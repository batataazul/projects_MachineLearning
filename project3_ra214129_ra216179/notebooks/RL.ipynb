{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gabriel/Documentos/Tarefas/Universidade/MC886/projects_MachineLearning/project3_ra214129_ra216179/search/search\n"
     ]
    }
   ],
   "source": [
    "%cd ../search/search\n",
    "#! python pacman.py --layout originalClassic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pacman import runGames, readCommand\n",
    "from game import Agent, Directions\n",
    "from util import manhattanDistance\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getState(state):\n",
    "    currentState = (state.getPacmanPosition(),Qlearn.getNearestFoodDis(state),state.getNumFood() + len(state.getCapsules()),Qlearn.nearestGhostPos(state))\n",
    "    return currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Agent):\n",
    "    def __init__(self, alpha,gamma,epsilon = 1,Qtable = {}):\n",
    "        self.Qtable = Qtable\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.score = 0\n",
    "        self.action = None\n",
    "        self.currentState = None\n",
    "        self.actions = []\n",
    "        self.currentActions = 0\n",
    "        self.rewards = []\n",
    "        self.reward = 0\n",
    "        self.scores = []\n",
    "    \n",
    "    def getQvalue(self,state,action):\n",
    "        pair = str((state,action))\n",
    "        return self.Qtable.get(pair,0.0)\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        actions = state.getLegalPacmanActions()\n",
    "        self.score = state.getScore()\n",
    "        currentState = getState(state)\n",
    "        action = self.getBestAction(currentState,actions)\n",
    "        state2 = state.generatePacmanSuccessor(action)\n",
    "        nextState = getState(state2)\n",
    "        if state2.isLose():\n",
    "            reinforcement = -20000\n",
    "        elif state2.isWin():\n",
    "            reinforcement = 20000\n",
    "        else:\n",
    "            reinforcement = self.getReward(currentState,nextState,state2.getScore())\n",
    "        self.learn(currentState,nextState,action,actions,reinforcement)\n",
    "        self.action = action\n",
    "        self.currentState = currentState\n",
    "        self.currentActions += 1\n",
    "        self.reward += reinforcement\n",
    "        return action\n",
    "        \n",
    "        \n",
    "    def nearestGhostPos(state):\n",
    "        ghostList = state.getGhostPositions()\n",
    "        nearestDistance = Qlearn.getNearGhost(state.getPacmanPosition(),ghostList)\n",
    "        return nearestDistance\n",
    "\n",
    "    def getNearGhost(pacman,ghosts):\n",
    "        minDis = float(\"inf\")\n",
    "        minPos = -1\n",
    "        for i in range(len(ghosts)):\n",
    "            distance = manhattanDistance(pacman,ghosts[i])\n",
    "            if distance < minDis:\n",
    "                minDis = distance\n",
    "                minPos = i\n",
    "        if minPos > -1:\n",
    "            return ghosts[minPos]\n",
    "        else:\n",
    "            return (-1,-1)\n",
    "\n",
    "    def getNearestFoodDis(state):\n",
    "        posList = Qlearn.getFoodPos(state.getFood())\n",
    "        posList = posList + state.getCapsules()\n",
    "        minDis = minDis = float(\"inf\")\n",
    "        #minPos = (-1,-1)\n",
    "        pacPos = state.getPacmanPosition()\n",
    "        for i in posList:\n",
    "            distance = manhattanDistance(pacPos,i)\n",
    "            if distance < minDis:\n",
    "                minDis = distance\n",
    "                #minPos = i\n",
    "        return minDis\n",
    "\n",
    "\n",
    "    def getFoodPos(grid):\n",
    "        posList = []\n",
    "        gridList = grid.asList()\n",
    "        for i in range(len(gridList)):\n",
    "            for j in range(len(gridList[i])):\n",
    "                if gridList[i][j]:\n",
    "                    posList.append((i,j))\n",
    "        return posList\n",
    "\n",
    "    def getBestAction(self,currentState,actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        maxValue = float(\"-inf\")\n",
    "        maxAction = []\n",
    "        for i in actions:\n",
    "            value = self.getQvalue(currentState,i)\n",
    "            if value > maxValue:\n",
    "                maxValue = value\n",
    "                maxAction = [i]\n",
    "            if value == maxValue:\n",
    "                maxAction.append(i)\n",
    "        #print(maxValue)\n",
    "        #print(maxAction)\n",
    "        return random.choice(maxAction)\n",
    "\n",
    "    def learn(self,currentState,nextState,action,actions,reinforcement):\n",
    "        nextActions = [self.getQvalue(nextState,a) for a in actions]\n",
    "        if len(nextActions) > 0:\n",
    "            newQ = max(nextActions)\n",
    "        else:\n",
    "            newQ = 0\n",
    "        currentQ = self.getQvalue(currentState,action)\n",
    "        self.Qtable[str((currentState,action))] = currentQ + self.alpha * (reinforcement + (self.gamma*newQ) - currentQ)\n",
    "\n",
    "    def setEpsilon(self,epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def serialize(self,path):\n",
    "        try:\n",
    "            f = open(path,\"w\")\n",
    "            json.dump(self.Qtable,f)\n",
    "            f.close()\n",
    "        except:\n",
    "            raise Exception(\"You've got an error\")\n",
    "\n",
    "    def desserialize(self,path):\n",
    "        try:\n",
    "            f = open(path,\"r\")\n",
    "            self.Qtable = json.load(f)\n",
    "            f.close()\n",
    "        except:\n",
    "            raise Exception(\"You've got an error\")\n",
    "    \n",
    "    def getReward(self,state1,state2,score):\n",
    "        pacGhostDistance1 = manhattanDistance(state1[0],state1[3])\n",
    "        pacGhostDistance2 = manhattanDistance(state2[0],state2[3])\n",
    "        #fDis1 = manhattanDistance(state1[0],state1[1])\n",
    "        #fDis2 = manhattanDistance(state2[0],state2[1])\n",
    "        eatenFood = state2[2] - state1[2]\n",
    "        ghostDistance = pacGhostDistance2 - pacGhostDistance1\n",
    "        foodDistance = state2[1] - state1[1]\n",
    "        if ghostDistance > 0:\n",
    "            ghostReward = ghostDistance*200\n",
    "        else:\n",
    "            ghostReward = 200*ghostDistance\n",
    "        if foodDistance < 0:\n",
    "            foodReward = -500 * foodDistance\n",
    "        else:\n",
    "            foodReward = 0\n",
    "        if eatenFood > 0:\n",
    "            eatReward = 500*eatenFood\n",
    "        else:\n",
    "            eatReward = 0\n",
    "        scoreDifference = score - self.score\n",
    "        if scoreDifference > 0:\n",
    "            scoreReward = 3*scoreDifference\n",
    "        else:\n",
    "            scoreReward = 5*scoreDifference\n",
    "        return ghostReward + foodReward + eatReward + scoreReward\n",
    "    \n",
    "    def win(self,terminal):\n",
    "        self.saveResult(terminal.state.getScore())\n",
    "        self.learn(self.currentState,getState(terminal.state),self.action,terminal.state.getLegalPacmanActions(),20000)\n",
    "    \n",
    "    def lose(self,terminal):\n",
    "        self.saveResult(terminal.state.getScore())\n",
    "        self.learn(self.currentState,getState(terminal.state),self.action,terminal.state.getLegalPacmanActions(),-20000)\n",
    "    \n",
    "    def saveResult(self, score):\n",
    "        self.actions.append(self.currentActions)\n",
    "        self.scores.append(score)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.currentActions = 0\n",
    "        self.reward = 0\n",
    "        self.score = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best:  \n",
    "GhostReward = 200/200  \n",
    "FoodReward = -500/0  \n",
    "EatReward = 500/0  \n",
    "ScoreReward = 3/5  \n",
    "Alpha = 0.85  \n",
    "Gamma = 0.3  \n",
    "\n",
    "First Win: 3142  \n",
    "Total win:   \n",
    "Average: -344.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(agente,k):\n",
    "    for i in range(0,k-1,1000):\n",
    "        try:    \n",
    "            print(\"Episode \"+str(i+1)+\": Number of actions = \"+str(agente.actions[i])+\"; Total score = \"+str(agente.scores[i])+\"; Total Reward = \"+str(agente.rewards[i]))\n",
    "        except(IndexError):\n",
    "            print(\"Index out of range\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-q', '--pacman', 'LeftTurnAgent', '--layout', 'smallClassic']\n"
     ]
    }
   ],
   "source": [
    "args = readCommand([\"-q\", \"--pacman\", \"LeftTurnAgent\",\"--layout\",\"smallClassic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = Qlearn(0.85,0.3,epsilon = 1)\n",
    "args[\"pacman\"] = agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacman emerges victorious! Score: 759\n",
      "3142\n",
      "Pacman emerges victorious! Score: 736\n",
      "9349\n",
      "Pacman emerges victorious! Score: 886\n",
      "12041\n",
      "Pacman emerges victorious! Score: 819\n",
      "12103\n",
      "Pacman emerges victorious! Score: 671\n",
      "12667\n",
      "Pacman emerges victorious! Score: 1026\n",
      "12765\n",
      "Pacman emerges victorious! Score: 894\n",
      "13771\n",
      "Pacman emerges victorious! Score: 569\n",
      "14012\n",
      "Pacman emerges victorious! Score: 625\n",
      "14239\n",
      "Pacman emerges victorious! Score: 631\n",
      "14494\n",
      "Pacman emerges victorious! Score: 638\n",
      "15167\n",
      "Pacman emerges victorious! Score: 683\n",
      "16695\n",
      "Pacman emerges victorious! Score: 660\n",
      "17021\n",
      "Pacman emerges victorious! Score: 815\n",
      "17075\n",
      "Pacman emerges victorious! Score: 868\n",
      "17297\n",
      "Pacman emerges victorious! Score: 395\n",
      "17987\n",
      "Pacman emerges victorious! Score: 859\n",
      "18808\n",
      "Pacman emerges victorious! Score: 640\n",
      "20034\n",
      "Pacman emerges victorious! Score: 863\n",
      "20073\n",
      "Pacman emerges victorious! Score: 711\n",
      "20470\n",
      "Pacman emerges victorious! Score: 516\n",
      "20769\n",
      "Pacman emerges victorious! Score: 834\n",
      "21296\n",
      "Pacman emerges victorious! Score: 1012\n",
      "21392\n",
      "Pacman emerges victorious! Score: 865\n",
      "21629\n",
      "Pacman emerges victorious! Score: 697\n",
      "23051\n",
      "Pacman emerges victorious! Score: 796\n",
      "24048\n",
      "Pacman emerges victorious! Score: 759\n",
      "24335\n",
      "Pacman emerges victorious! Score: 641\n",
      "24478\n",
      "Pacman emerges victorious! Score: 885\n",
      "24555\n",
      "Pacman emerges victorious! Score: 712\n",
      "25422\n",
      "Pacman emerges victorious! Score: 782\n",
      "25664\n",
      "Pacman emerges victorious! Score: 623\n",
      "25784\n",
      "Pacman emerges victorious! Score: 476\n",
      "25887\n",
      "Pacman emerges victorious! Score: 753\n",
      "26425\n",
      "Pacman emerges victorious! Score: 1193\n",
      "26664\n",
      "Pacman emerges victorious! Score: 520\n",
      "26909\n",
      "Pacman emerges victorious! Score: 762\n",
      "27045\n",
      "Pacman emerges victorious! Score: 823\n",
      "27872\n",
      "Pacman emerges victorious! Score: 691\n",
      "28073\n",
      "Pacman emerges victorious! Score: 795\n",
      "28295\n",
      "Pacman emerges victorious! Score: 591\n",
      "28404\n",
      "Pacman emerges victorious! Score: 322\n",
      "28645\n",
      "Pacman emerges victorious! Score: 869\n",
      "28801\n",
      "Pacman emerges victorious! Score: 519\n",
      "28991\n",
      "Pacman emerges victorious! Score: 301\n",
      "29320\n",
      "Pacman emerges victorious! Score: 909\n",
      "29531\n",
      "Pacman emerges victorious! Score: 655\n",
      "29794\n",
      "Pacman emerges victorious! Score: 698\n",
      "30820\n",
      "Pacman emerges victorious! Score: 581\n",
      "31590\n",
      "Pacman emerges victorious! Score: 728\n",
      "32134\n",
      "Pacman emerges victorious! Score: 582\n",
      "32417\n",
      "Pacman emerges victorious! Score: 1180\n",
      "33862\n",
      "Pacman emerges victorious! Score: 561\n",
      "33985\n",
      "Pacman emerges victorious! Score: 878\n",
      "34827\n",
      "Pacman emerges victorious! Score: 600\n",
      "35200\n",
      "Pacman emerges victorious! Score: 643\n",
      "35262\n",
      "Pacman emerges victorious! Score: 360\n",
      "35277\n",
      "Pacman emerges victorious! Score: 1013\n",
      "35375\n",
      "Pacman emerges victorious! Score: 871\n",
      "35642\n",
      "Pacman emerges victorious! Score: 613\n",
      "35868\n",
      "Pacman emerges victorious! Score: 557\n",
      "35985\n",
      "Pacman emerges victorious! Score: 613\n",
      "36760\n",
      "Pacman emerges victorious! Score: 662\n",
      "36772\n",
      "Pacman emerges victorious! Score: 759\n",
      "36921\n",
      "Pacman emerges victorious! Score: 495\n",
      "36979\n",
      "Pacman emerges victorious! Score: 598\n",
      "37251\n",
      "Pacman emerges victorious! Score: 713\n",
      "37275\n",
      "Pacman emerges victorious! Score: 799\n",
      "37451\n",
      "Pacman emerges victorious! Score: 782\n",
      "37555\n",
      "Pacman emerges victorious! Score: 766\n",
      "37924\n",
      "Pacman emerges victorious! Score: 813\n",
      "37925\n",
      "Pacman emerges victorious! Score: 585\n",
      "38243\n",
      "Pacman emerges victorious! Score: 784\n",
      "38635\n",
      "Pacman emerges victorious! Score: 721\n",
      "39502\n"
     ]
    }
   ],
   "source": [
    "victorys = []\n",
    "k = 40000\n",
    "summ = 0\n",
    "for i in range(k):\n",
    "    agente.setEpsilon(1/(i+1))\n",
    "    a.append(runGames(**args))\n",
    "    if a[i][0].state.isLose():\n",
    "        agente.lose(a[i][0])\n",
    "    elif a[i][0].state.isWin():\n",
    "        agente.win(a[i][0])\n",
    "        victorys.append(a[i])\n",
    "        print(i)\n",
    "    summ += a[i][0].state.getScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Number of actions = 14; Total score = -454.0; Total Reward = -19073.0\n",
      "Episode 1001: Number of actions = 142; Total score = -62.0; Total Reward = 22808.0\n",
      "Episode 2001: Number of actions = 195; Total score = -385.0; Total Reward = 22817.0\n",
      "Episode 3001: Number of actions = 568; Total score = -608.0; Total Reward = 35867.0\n",
      "Episode 4001: Number of actions = 167; Total score = -407.0; Total Reward = 29797.0\n",
      "Episode 5001: Number of actions = 352; Total score = -392.0; Total Reward = 48812.0\n",
      "Episode 6001: Number of actions = 97; Total score = -207.0; Total Reward = 16425.0\n",
      "Episode 7001: Number of actions = 113; Total score = -293.0; Total Reward = 18059.0\n",
      "Episode 8001: Number of actions = 166; Total score = -256.0; Total Reward = 24944.0\n",
      "Episode 9001: Number of actions = 31; Total score = -421.0; Total Reward = 4497.0\n",
      "Episode 10001: Number of actions = 125; Total score = -205.0; Total Reward = 1592.0\n",
      "Episode 11001: Number of actions = 154; Total score = -444.0; Total Reward = 24902.0\n",
      "Episode 12001: Number of actions = 95; Total score = -205.0; Total Reward = 11535.0\n",
      "Episode 13001: Number of actions = 124; Total score = -334.0; Total Reward = 17608.0\n",
      "Episode 14001: Number of actions = 148; Total score = -268.0; Total Reward = 21376.0\n",
      "Episode 15001: Number of actions = 28; Total score = -398.0; Total Reward = 4976.0\n",
      "Episode 16001: Number of actions = 175; Total score = -205.0; Total Reward = 34229.0\n",
      "Episode 17001: Number of actions = 208; Total score = -408.0; Total Reward = 16020.0\n",
      "Episode 18001: Number of actions = 305; Total score = -355.0; Total Reward = 37777.0\n",
      "Episode 19001: Number of actions = 34; Total score = -404.0; Total Reward = 8246.0\n",
      "Episode 20001: Number of actions = 266; Total score = -296.0; Total Reward = 56774.0\n",
      "Episode 21001: Number of actions = 106; Total score = -276.0; Total Reward = -2669.0\n",
      "Episode 22001: Number of actions = 149; Total score = -339.0; Total Reward = 2852.0\n",
      "Episode 23001: Number of actions = 65; Total score = -445.0; Total Reward = 6559.0\n",
      "Episode 24001: Number of actions = 163; Total score = -273.0; Total Reward = 34433.0\n",
      "Episode 25001: Number of actions = 105; Total score = -215.0; Total Reward = 9383.0\n",
      "Episode 26001: Number of actions = 27; Total score = -387.0; Total Reward = 4013.0\n",
      "Episode 27001: Number of actions = 72; Total score = -432.0; Total Reward = 12188.0\n",
      "Episode 28001: Number of actions = 336; Total score = -446.0; Total Reward = 43868.0\n",
      "Episode 29001: Number of actions = 122; Total score = -382.0; Total Reward = 17458.0\n",
      "Episode 30001: Number of actions = 138; Total score = -248.0; Total Reward = 17520.0\n",
      "Episode 31001: Number of actions = 119; Total score = -359.0; Total Reward = 12137.0\n",
      "Episode 32001: Number of actions = 76; Total score = -416.0; Total Reward = 12732.0\n",
      "Episode 33001: Number of actions = 159; Total score = -469.0; Total Reward = 28813.0\n",
      "Episode 34001: Number of actions = 177; Total score = -267.0; Total Reward = 26727.0\n",
      "Episode 35001: Number of actions = 65; Total score = -365.0; Total Reward = 14415.0\n",
      "Episode 36001: Number of actions = 321; Total score = 39.0; Total Reward = 44971.0\n",
      "Episode 37001: Number of actions = 161; Total score = -211.0; Total Reward = 29797.0\n",
      "Episode 38001: Number of actions = 282; Total score = -302.0; Total Reward = 37626.0\n",
      "Episode 39001: Number of actions = 104; Total score = -324.0; Total Reward = 16976.0\n"
     ]
    }
   ],
   "source": [
    "showResults(agente,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.serialize(\"../../notebooks/results/small/a085-g03-d100721.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "-344.1665\n"
     ]
    }
   ],
   "source": [
    "print(len(victorys))\n",
    "print(summ/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-q', '--pacman', 'LeftTurnAgent', '--layout', 'mediumClassic']\n"
     ]
    }
   ],
   "source": [
    "argsM = readCommand([\"-q\", \"--pacman\", \"LeftTurnAgent\",\"--layout\",\"mediumClassic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "aM = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteM = Qlearn(0.85,0.3,epsilon = 1)\n",
    "argsM[\"pacman\"] = agenteM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victorysM = []\n",
    "kM = 80000\n",
    "summM = 0\n",
    "for i in range(kM):\n",
    "    agenteM.setEpsilon(1/(i+1))\n",
    "    aM.append(runGames(**argsM))\n",
    "    if aM[i][0].state.isLose():\n",
    "        agenteM.lose(aM[i][0])\n",
    "    elif aM[i][0].state.isWin():\n",
    "        agenteM.win(aM[i][0])\n",
    "        victorysM.append(aM[i])\n",
    "        print(i)\n",
    "    summM += aM[i][0].state.getScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteM.serialize(\"../../notebooks/results/medium/a085-g03-d090721.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(victorysM))\n",
    "print(summM/kO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argsO = readCommand([\"-q\", \"--pacman\", \"LeftTurnAgent\",\"--layout\",\"originalClassic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aO = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteO = Qlearn(0.85,0.3,epsilon = 1)\n",
    "argsO[\"pacman\"] = agenteO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victorysO = []\n",
    "kO = 120000\n",
    "summO = 0\n",
    "for i in range(kO):\n",
    "    agenteO.setEpsilon(1/(i+1))\n",
    "    aO.append(runGames(**argsO))\n",
    "    if aO[i][0].state.isLose():\n",
    "        agenteO.lose(aO[i][0])\n",
    "    elif aO[i][0].state.isWin():\n",
    "        agenteO.win(aO[i][0])\n",
    "        victorysO.append(aO[i])\n",
    "        print(i)\n",
    "    summ0 += aO[i][0].state.getScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteO.serialize(\"../../notebooks/results/original/a085-g03-d090721.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(victorysO))\n",
    "print(summO/kO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class smartPacman(Agent):\n",
    "    def __init__(self, path):\n",
    "        self.desserialize(path)\n",
    "        self.score = 0\n",
    "        self.action = 0\n",
    "    \n",
    "    def getQvalue(self,state,action):\n",
    "        pair = str((state,action))\n",
    "        return self.Qtable.get(pair,0.0)\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        actions = state.getLegalPacmanActions()\n",
    "        currentState = getState(state)\n",
    "        action = self.getBestAction(currentState,actions)\n",
    "        return action\n",
    "    \n",
    "    def getBestAction(self,currentState,actions):\n",
    "        maxValue = float(\"-inf\")\n",
    "        maxAction = []\n",
    "        for i in actions:\n",
    "            value = self.getQvalue(currentState,i)\n",
    "            if value > maxValue:\n",
    "                maxValue = value\n",
    "                maxAction = [i]\n",
    "            if value == maxValue:\n",
    "                maxAction.append(i)\n",
    "        #print(maxValue)\n",
    "        #print(maxAction)\n",
    "        self.action += 1\n",
    "        return random.choice(maxAction)\n",
    "    \n",
    "    def desserialize(self,path):\n",
    "        try:\n",
    "            f = open(path,\"r\")\n",
    "            self.Qtable = json.load(f)\n",
    "            f.close()\n",
    "        except:\n",
    "            raise Exception(\"You've got an error\")\n",
    "    def finish(self,state):\n",
    "        self.score = state.getScore()\n",
    "        self.action = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--pacman', 'LeftTurnAgent', '--layout', 'smallClassic']\n"
     ]
    }
   ],
   "source": [
    "#agenteEsperto = smartPacman(\"../../notebooks/results/small/a085-g03-d090721-2.json\")\n",
    "agenteEsperto = smartPacman(\"../../notebooks/results/small/a085-g03-d090721-2.json\")\n",
    "argsV = readCommand([\"--pacman\", \"LeftTurnAgent\",\"--layout\",\"smallClassic\"])\n",
    "argsV[\"pacman\"] = agenteEsperto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "-468.0\n"
     ]
    }
   ],
   "source": [
    "a = runGames(**argsV)\n",
    "print(a[0].state.isWin())\n",
    "print(a[0].state.getScore())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a33e6643d3d266930a9315102c951dbbb0201b743375bf7e85bc0f4d5308486"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
