{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gabriel/Documentos/Tarefas/Universidade/MC886/projects_MachineLearning/project3_ra214129_ra216179/search/search\n"
     ]
    }
   ],
   "source": [
    "%cd ../search/search\n",
    "#! python pacman.py --layout originalClassic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pacman import runGames, readCommand\n",
    "from game import Agent, Directions\n",
    "from util import manhattanDistance\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getState(state):\n",
    "    currentState = (state.getPacmanPosition(),Qlearn.getNearestFoodDis(state),state.getNumFood() + len(state.getCapsules()),Qlearn.nearestGhostPos(state))\n",
    "    return currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Agent):\n",
    "    def __init__(self, alpha,gamma,epsilon = 1,Qtable = {}):\n",
    "        self.Qtable = Qtable\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.score = 0\n",
    "        self.action = None\n",
    "        self.currentState = None\n",
    "        self.actions = []\n",
    "        self.currentActions = 0\n",
    "        self.rewards = []\n",
    "        self.reward = 0\n",
    "        self.scores = []\n",
    "    \n",
    "    def getQvalue(self,state,action):\n",
    "        pair = str((state,action))\n",
    "        return self.Qtable.get(pair,0.0)\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        actions = state.getLegalPacmanActions()\n",
    "        self.score = state.getScore()\n",
    "        currentState = getState(state)\n",
    "        action = self.getBestAction(currentState,actions)\n",
    "        state2 = state.generatePacmanSuccessor(action)\n",
    "        nextState = getState(state2)\n",
    "        if state2.isLose():\n",
    "            reinforcement = -20000\n",
    "        elif state2.isWin():\n",
    "            reinforcement = 20000\n",
    "        else:\n",
    "            reinforcement = self.getReward(currentState,nextState,state2.getScore())\n",
    "        self.learn(currentState,nextState,action,actions,reinforcement)\n",
    "        self.action = action\n",
    "        self.currentState = currentState\n",
    "        self.currentActions += 1\n",
    "        self.reward += reinforcement\n",
    "        return action\n",
    "        \n",
    "        \n",
    "    def nearestGhostPos(state):\n",
    "        ghostList = state.getGhostPositions()\n",
    "        nearestDistance = Qlearn.getNearGhost(state.getPacmanPosition(),ghostList)\n",
    "        return nearestDistance\n",
    "\n",
    "    def getNearGhost(pacman,ghosts):\n",
    "        minDis = float(\"inf\")\n",
    "        minPos = -1\n",
    "        for i in range(len(ghosts)):\n",
    "            distance = manhattanDistance(pacman,ghosts[i])\n",
    "            if distance < minDis:\n",
    "                minDis = distance\n",
    "                minPos = i\n",
    "        if minPos > -1:\n",
    "            return ghosts[minPos]\n",
    "        else:\n",
    "            return (-1,-1)\n",
    "\n",
    "    def getNearestFoodDis(state):\n",
    "        posList = Qlearn.getFoodPos(state.getFood())\n",
    "        posList = posList + state.getCapsules()\n",
    "        minDis = minDis = float(\"inf\")\n",
    "        #minPos = (-1,-1)\n",
    "        pacPos = state.getPacmanPosition()\n",
    "        for i in posList:\n",
    "            distance = manhattanDistance(pacPos,i)\n",
    "            if distance < minDis:\n",
    "                minDis = distance\n",
    "                #minPos = i\n",
    "        return minDis\n",
    "\n",
    "\n",
    "    def getFoodPos(grid):\n",
    "        posList = []\n",
    "        gridList = grid.asList()\n",
    "        for i in range(len(gridList)):\n",
    "            for j in range(len(gridList[i])):\n",
    "                if gridList[i][j]:\n",
    "                    posList.append((i,j))\n",
    "        return posList\n",
    "\n",
    "    def getBestAction(self,currentState,actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        maxValue = float(\"-inf\")\n",
    "        maxAction = []\n",
    "        for i in actions:\n",
    "            value = self.getQvalue(currentState,i)\n",
    "            if value > maxValue:\n",
    "                maxValue = value\n",
    "                maxAction = [i]\n",
    "            if value == maxValue:\n",
    "                maxAction.append(i)\n",
    "        #print(maxValue)\n",
    "        #print(maxAction)\n",
    "        return random.choice(maxAction)\n",
    "\n",
    "    def learn(self,currentState,nextState,action,actions,reinforcement):\n",
    "        nextActions = [self.getQvalue(nextState,a) for a in actions]\n",
    "        if len(nextActions) > 0:\n",
    "            newQ = max(nextActions)\n",
    "        else:\n",
    "            newQ = 0\n",
    "        currentQ = self.getQvalue(currentState,action)\n",
    "        self.Qtable[str((currentState,action))] = currentQ + self.alpha * (reinforcement + (self.gamma*newQ) - currentQ)\n",
    "\n",
    "    def setEpsilon(self,epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def serialize(self,path):\n",
    "        try:\n",
    "            f = open(path,\"w\")\n",
    "            json.dump(self.Qtable,f)\n",
    "            f.close()\n",
    "        except:\n",
    "            raise Exception(\"You've got an error\")\n",
    "\n",
    "    def desserialize(self,path):\n",
    "        try:\n",
    "            f = open(path,\"r\")\n",
    "            self.Qtable = json.load(f)\n",
    "            f.close()\n",
    "        except:\n",
    "            raise Exception(\"You've got an error\")\n",
    "    \n",
    "    def getReward(self,state1,state2,score):\n",
    "        pacGhostDistance1 = manhattanDistance(state1[0],state1[3])\n",
    "        pacGhostDistance2 = manhattanDistance(state2[0],state2[3])\n",
    "        #fDis1 = manhattanDistance(state1[0],state1[1])\n",
    "        #fDis2 = manhattanDistance(state2[0],state2[1])\n",
    "        eatenFood = state2[2] - state1[2]\n",
    "        ghostDistance = pacGhostDistance2 - pacGhostDistance1\n",
    "        foodDistance = state2[1] - state1[1]\n",
    "        if ghostDistance > 0:\n",
    "            ghostReward = ghostDistance*100\n",
    "        else:\n",
    "            ghostReward = 100*ghostDistance\n",
    "        if foodDistance < 0:\n",
    "            foodReward = -400 * foodDistance\n",
    "        else:\n",
    "            foodReward = 0\n",
    "        if eatenFood > 0:\n",
    "            eatReward = 400*eatenFood\n",
    "        else:\n",
    "            eatReward = 0\n",
    "        scoreDifference = score - self.score\n",
    "        if scoreDifference > 0:\n",
    "            scoreReward = 30*scoreDifference\n",
    "        else:\n",
    "            scoreReward = 5*scoreDifference\n",
    "        return ghostReward + foodReward + eatReward + scoreReward\n",
    "    \n",
    "    def win(self,terminal):\n",
    "        self.saveResult(terminal.state.getScore())\n",
    "        self.learn(self.currentState,getState(terminal.state),self.action,terminal.state.getLegalPacmanActions(),20000)\n",
    "    \n",
    "    def lose(self,terminal):\n",
    "        self.saveResult(terminal.state.getScore())\n",
    "        self.learn(self.currentState,getState(terminal.state),self.action,terminal.state.getLegalPacmanActions(),-20000)\n",
    "    \n",
    "    def saveResult(self, score):\n",
    "        self.actions.append(self.currentActions)\n",
    "        self.scores.append(score)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.currentActions = 0\n",
    "        self.reward = 0\n",
    "        self.score = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best:  \n",
    "GhostReward = 200/200  \n",
    "FoodReward = -400/0  \n",
    "EatReward = 400/0  \n",
    "ScoreReward = 3/1  \n",
    "Alpha = 0.85  \n",
    "Gamma = 0.3  \n",
    "\n",
    "First Win: 5760  \n",
    "Total win: 54  \n",
    "Average: -344.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(agente,k):\n",
    "    for i in range(0,k-1,1000):\n",
    "        try:    \n",
    "            print(\"Episode \"+str(i+1)+\": Number of actions = \"+str(agente.actions[i])+\"; Total score = \"+str(agente.scores[i])+\"; Total Reward = \"+str(agente.rewards[i]))\n",
    "        except(IndexError):\n",
    "            print(\"Index out of range\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-q', '--pacman', 'LeftTurnAgent', '--layout', 'smallClassic']\n"
     ]
    }
   ],
   "source": [
    "args = readCommand([\"-q\", \"--pacman\", \"LeftTurnAgent\",\"--layout\",\"smallClassic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = Qlearn(0.85,0.3,epsilon = 1)\n",
    "args[\"pacman\"] = agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacman emerges victorious! Score: 606\n",
      "11587\n",
      "Pacman emerges victorious! Score: 522\n",
      "12004\n",
      "Pacman emerges victorious! Score: 761\n",
      "13134\n",
      "Pacman emerges victorious! Score: 709\n",
      "17251\n",
      "Pacman emerges victorious! Score: 672\n",
      "19205\n",
      "Pacman emerges victorious! Score: 889\n",
      "20021\n",
      "Pacman emerges victorious! Score: 843\n",
      "21756\n",
      "Pacman emerges victorious! Score: 937\n",
      "22344\n",
      "Pacman emerges victorious! Score: 951\n",
      "24905\n",
      "Pacman emerges victorious! Score: 736\n",
      "25505\n",
      "Pacman emerges victorious! Score: 721\n",
      "25547\n",
      "Pacman emerges victorious! Score: 924\n",
      "27064\n",
      "Pacman emerges victorious! Score: 671\n",
      "27421\n",
      "Pacman emerges victorious! Score: 818\n",
      "27840\n",
      "Pacman emerges victorious! Score: 605\n",
      "28319\n",
      "Pacman emerges victorious! Score: 678\n",
      "28647\n",
      "Pacman emerges victorious! Score: 972\n",
      "30290\n",
      "Pacman emerges victorious! Score: 433\n",
      "30644\n",
      "Pacman emerges victorious! Score: 758\n",
      "31393\n",
      "Pacman emerges victorious! Score: 936\n",
      "31451\n",
      "Pacman emerges victorious! Score: 770\n",
      "31822\n",
      "Pacman emerges victorious! Score: 1120\n",
      "32065\n",
      "Pacman emerges victorious! Score: 429\n",
      "33668\n",
      "Pacman emerges victorious! Score: 798\n",
      "35717\n",
      "Pacman emerges victorious! Score: 981\n",
      "36731\n",
      "Pacman emerges victorious! Score: 722\n",
      "36930\n",
      "Pacman emerges victorious! Score: 617\n",
      "37335\n",
      "Pacman emerges victorious! Score: 805\n",
      "38577\n",
      "Pacman emerges victorious! Score: 618\n",
      "38661\n",
      "Pacman emerges victorious! Score: 1047\n",
      "39258\n"
     ]
    }
   ],
   "source": [
    "victorys = []\n",
    "k = 40000\n",
    "summ = 0\n",
    "for i in range(k):\n",
    "    agente.setEpsilon(1/(i+1))\n",
    "    a.append(runGames(**args))\n",
    "    if a[i][0].state.isLose():\n",
    "        agente.lose(a[i][0])\n",
    "    elif a[i][0].state.isWin():\n",
    "        agente.win(a[i][0])\n",
    "        victorys.append(a[i])\n",
    "        print(i)\n",
    "    summ += a[i][0].state.getScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Number of actions = 23; Total score = -403.0; Total Reward = -16110.0\n",
      "Episode 1001: Number of actions = 36; Total score = -406.0; Total Reward = 5895.0\n",
      "Episode 2001: Number of actions = 167; Total score = -407.0; Total Reward = 25515.0\n",
      "Episode 3001: Number of actions = 236; Total score = -546.0; Total Reward = 36645.0\n",
      "Episode 4001: Number of actions = 30; Total score = -320.0; Total Reward = 9725.0\n",
      "Episode 5001: Number of actions = 31; Total score = -401.0; Total Reward = 8020.0\n",
      "Episode 6001: Number of actions = 165; Total score = -45.0; Total Reward = 41225.0\n",
      "Episode 7001: Number of actions = 107; Total score = -197.0; Total Reward = 3320.0\n",
      "Episode 8001: Number of actions = 198; Total score = -328.0; Total Reward = 32685.0\n",
      "Episode 9001: Number of actions = 253; Total score = -513.0; Total Reward = 30135.0\n",
      "Episode 10001: Number of actions = 215; Total score = -495.0; Total Reward = 31375.0\n",
      "Episode 11001: Number of actions = 84; Total score = -194.0; Total Reward = 17330.0\n",
      "Episode 12001: Number of actions = 80; Total score = -370.0; Total Reward = 13875.0\n",
      "Episode 13001: Number of actions = 170; Total score = -270.0; Total Reward = 27875.0\n",
      "Episode 14001: Number of actions = 97; Total score = -417.0; Total Reward = 16165.0\n",
      "Episode 15001: Number of actions = 164; Total score = -424.0; Total Reward = 22180.0\n",
      "Episode 16001: Number of actions = 81; Total score = -451.0; Total Reward = 12070.0\n",
      "Episode 17001: Number of actions = 24; Total score = -364.0; Total Reward = 7580.0\n",
      "Episode 18001: Number of actions = 103; Total score = -193.0; Total Reward = 19185.0\n",
      "Episode 19001: Number of actions = 188; Total score = -428.0; Total Reward = 24610.0\n",
      "Episode 20001: Number of actions = 155; Total score = -405.0; Total Reward = 23700.0\n",
      "Episode 21001: Number of actions = 176; Total score = -456.0; Total Reward = 17070.0\n",
      "Episode 22001: Number of actions = 221; Total score = -391.0; Total Reward = 30170.0\n",
      "Episode 23001: Number of actions = 246; Total score = -556.0; Total Reward = 19495.0\n",
      "Episode 24001: Number of actions = 71; Total score = -171.0; Total Reward = 17520.0\n",
      "Episode 25001: Number of actions = 108; Total score = -368.0; Total Reward = 16560.0\n",
      "Episode 26001: Number of actions = 288; Total score = -408.0; Total Reward = 11890.0\n",
      "Episode 27001: Number of actions = 68; Total score = -428.0; Total Reward = 7010.0\n",
      "Episode 28001: Number of actions = 61; Total score = -401.0; Total Reward = 11495.0\n",
      "Episode 29001: Number of actions = 374; Total score = -504.0; Total Reward = 32505.0\n",
      "Episode 30001: Number of actions = 67; Total score = -377.0; Total Reward = 15690.0\n",
      "Episode 31001: Number of actions = 273; Total score = -163.0; Total Reward = 43135.0\n",
      "Episode 32001: Number of actions = 31; Total score = -321.0; Total Reward = 10920.0\n",
      "Episode 33001: Number of actions = 167; Total score = -277.0; Total Reward = 16465.0\n",
      "Episode 34001: Number of actions = 217; Total score = -337.0; Total Reward = 26290.0\n",
      "Episode 35001: Number of actions = 59; Total score = -369.0; Total Reward = 12530.0\n",
      "Episode 36001: Number of actions = 141; Total score = -231.0; Total Reward = 26670.0\n",
      "Episode 37001: Number of actions = 125; Total score = -225.0; Total Reward = 23650.0\n",
      "Episode 38001: Number of actions = 124; Total score = -494.0; Total Reward = 5755.0\n",
      "Episode 39001: Number of actions = 162; Total score = -322.0; Total Reward = 21140.0\n"
     ]
    }
   ],
   "source": [
    "showResults(agente,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.serialize(\"../../notebooks/results/small/a085-g03-d090721-4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "-339.8389\n"
     ]
    }
   ],
   "source": [
    "print(len(victorys))\n",
    "print(summ/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-q', '--pacman', 'LeftTurnAgent', '--layout', 'mediumClassic']\n"
     ]
    }
   ],
   "source": [
    "argsM = readCommand([\"-q\", \"--pacman\", \"LeftTurnAgent\",\"--layout\",\"mediumClassic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "aM = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteM = Qlearn(0.85,0.3,epsilon = 1)\n",
    "argsM[\"pacman\"] = agenteM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victorysM = []\n",
    "kM = 80000\n",
    "summM = 0\n",
    "for i in range(kM):\n",
    "    agenteM.setEpsilon(1/(i+1))\n",
    "    aM.append(runGames(**argsM))\n",
    "    if aM[i][0].state.isLose():\n",
    "        agenteM.lose(aM[i][0])\n",
    "    elif aM[i][0].state.isWin():\n",
    "        agenteM.win(aM[i][0])\n",
    "        victorysM.append(aM[i])\n",
    "        print(i)\n",
    "    summM += aM[i][0].state.getScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteM.serialize(\"../../notebooks/results/medium/a085-g03-d090721.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(victorysM))\n",
    "print(summM/kO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argsO = readCommand([\"-q\", \"--pacman\", \"LeftTurnAgent\",\"--layout\",\"originalClassic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aO = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteO = Qlearn(0.85,0.3,epsilon = 1)\n",
    "argsO[\"pacman\"] = agenteO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victorysO = []\n",
    "kO = 120000\n",
    "summO = 0\n",
    "for i in range(kO):\n",
    "    agenteO.setEpsilon(1/(i+1))\n",
    "    aO.append(runGames(**argsO))\n",
    "    if aO[i][0].state.isLose():\n",
    "        agenteO.lose(aO[i][0])\n",
    "    elif aO[i][0].state.isWin():\n",
    "        agenteO.win(aO[i][0])\n",
    "        victorysO.append(aO[i])\n",
    "        print(i)\n",
    "    summ0 += aO[i][0].state.getScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenteO.serialize(\"../../notebooks/results/original/a085-g03-d090721.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(victorysO))\n",
    "print(summO/kO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class smartPacman(Agent):\n",
    "    def __init__(self, path):\n",
    "        self.desserialize(path)\n",
    "        self.score = 0\n",
    "        self.action = 0\n",
    "    \n",
    "    def getQvalue(self,state,action):\n",
    "        pair = str((state,action))\n",
    "        return self.Qtable.get(pair,0.0)\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        actions = state.getLegalPacmanActions()\n",
    "        currentState = getState(state)\n",
    "        action = self.getBestAction(currentState,actions)\n",
    "        return action\n",
    "    \n",
    "    def getBestAction(self,currentState,actions):\n",
    "        maxValue = float(\"-inf\")\n",
    "        maxAction = []\n",
    "        for i in actions:\n",
    "            value = self.getQvalue(currentState,i)\n",
    "            if value > maxValue:\n",
    "                maxValue = value\n",
    "                maxAction = [i]\n",
    "            if value == maxValue:\n",
    "                maxAction.append(i)\n",
    "        #print(maxValue)\n",
    "        #print(maxAction)\n",
    "        self.action += 1\n",
    "        return random.choice(maxAction)\n",
    "    \n",
    "    def desserialize(self,path):\n",
    "        try:\n",
    "            f = open(path,\"r\")\n",
    "            self.Qtable = json.load(f)\n",
    "            f.close()\n",
    "        except:\n",
    "            raise Exception(\"You've got an error\")\n",
    "    def finish(self,state):\n",
    "        self.score = state.getScore()\n",
    "        self.action = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--pacman', 'LeftTurnAgent', '--layout', 'smallClassic']\n"
     ]
    }
   ],
   "source": [
    "#agenteEsperto = smartPacman(\"../../notebooks/results/small/a085-g03-d090721-2.json\")\n",
    "agenteEsperto = smartPacman(\"../../notebooks/results/small/a085-g03-d090721-2.json\")\n",
    "argsV = readCommand([\"--pacman\", \"LeftTurnAgent\",\"--layout\",\"smallClassic\"])\n",
    "argsV[\"pacman\"] = agenteEsperto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "-468.0\n"
     ]
    }
   ],
   "source": [
    "a = runGames(**argsV)\n",
    "print(a[0].state.isWin())\n",
    "print(a[0].state.getScore())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a33e6643d3d266930a9315102c951dbbb0201b743375bf7e85bc0f4d5308486"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
