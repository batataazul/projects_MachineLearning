{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-durham",
   "metadata": {},
   "source": [
    "# Projeto 1: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-church",
   "metadata": {},
   "source": [
    "## Grupo\n",
    "Nomes: Camila Moraes Brito, Gabriel de Freitas Garcia  \n",
    "Ra: , 216179"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-venture",
   "metadata": {},
   "source": [
    "# Tratamento dos Dados\n",
    "\n",
    "Foram usados dois datasets em nosso projeto, eles podem ser encontrados em: \"../data\". O primerio foi um dataset de duas dimensões fornecido pela professora, o segundo foi um dataset sobre ataques cardíacos encontrado em [Dataset ataque cardíaco](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset). Em ambos os datasets optou-se por fazer fazer a leitura dos dados e depois seriaizá-los em formato JSON para evitar execuções desnecessárias do algoritmo de recuperação dos dados e por JSON ser mais fácil de recuperar que os formatos fornecidos. O padrão dos objetos JSON será exibido a seguir:  \n",
    "![teacher data](teacherData.png)\n",
    "![stroke data](strokeData.png)  \n",
    "<b>Imagem 1 e 2:</b> Formato dos objetos no JSON serializado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-gazette",
   "metadata": {},
   "source": [
    "Os notebooks onde este tratamento é feito se encontram em [Teacher Data](../Notebooks/parsingData1.ipynb) e [Stroke Data](../Notebooks/parsingData1.ipynb). Para o dataset fornecido o tratamento foi apenas verificar se havia dados não numéricos entre os dados corretos, por meio de tratamento de exceção. Para o outro dataset o tratamento será explicado em detalhes mais abaixo. Nesta etapa nenhum dado foi normalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-natural",
   "metadata": {},
   "source": [
    "O dataset sobre ataques cardíacos possui 12 features, uma delas, porém, o ID, representa um identificador único de cada indivíduo. Essa feature abrange uma gama muito grande de valores que não possui, provavelmente, nenhum significado e não serve para classificação dos elementos, ou seja, não é relevante para a análise e pode prejudicá-la, portanto optou-se por desprezá-la. As outras features e o tipo de dado que contem são: gênero - classificação (Homem, mulher ou outro), idade - inteiro, hipertensão - booleano, doença cardíaca - booleano, já foi casado - booleano, tipo de trabalho - classificação (Criança, desempregado, autônomo, funcionário privado ou funcionário público), tipo de habitação - classificação (Urbana ou Privada), nível médio de glicose - número contínuo, índice de massa corpórea - número contínuo, status de fumo - classificação (nunca fumou, ex-fumante, fumante ou desconhecido), ataque cardíaco - booleano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-guinea",
   "metadata": {},
   "source": [
    "Tendo em vista que os métodos de aprendizado não supervisionado utilizados precisam de números para trabalhar, todos os dados que não fossem numéricos precisaram ser adaptados, utilizando uma codificação desenvolvida por um dos membros da equipe que será explicada abaixo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-problem",
   "metadata": {},
   "source": [
    "Gênero:  \n",
    "Mulher : 1  \n",
    "Outro : 2  \n",
    "Homem : 3  \n",
    "\n",
    "Essa decisão foi tomada com base numa ideia de proximidade, Mulher e homem são diferentes e opostos em termos classificatórios já outro, o desenvolvedor da codificação optou por não entrar em discussões que fogem ao escopo do projeto, porém interpretou que está igualmente perto de homem ou mulher, sem privilegiar nenhum. Por isso foi escolhidos os valores referenciados acima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-break",
   "metadata": {},
   "source": [
    "Tipo de trabalho:  \n",
    "Criança : 0  \n",
    "Desempregado : 1  \n",
    "Autônomo : 2  \n",
    "Funcionário privado : 3  \n",
    "Funcionário público : 4\n",
    "\n",
    "Neste caso, a classificação se deu pelo nível de formalidade da ocupação, quanto mais formal, maior o número, crianças ficam com zero pois não podem trabalhar. Desta forma, tipos de emprego mais próximos, em questão de formalidade, ficam mais próximos e os muito díspares ficam mais distantes entre si."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-gateway",
   "metadata": {},
   "source": [
    "Tipo de habitação:  \n",
    "Rural : 0  \n",
    "Urbano : 1  \n",
    "\n",
    "Essa feature é binária, portanto, 0 e 1 foi a escolha natural para esse caso, qualquer outros dois valores diferentes poderiam ser usados, esses porém mantém a consistência com as dimensões booleanas do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-warehouse",
   "metadata": {},
   "source": [
    "Status de fumo:\n",
    "Nunca fumou : 0  \n",
    "Parou de fumar: 1  \n",
    "Fuma : 2  \n",
    "Desconhecido : -1  \n",
    "\n",
    "Neste caso, fumantes e não fumantes são opções diametralmente opostas, com ex-fumante estando entre as duas opções por já ter fumado, porém não fumar mais. Há uma quarta opção, desconhecido, que significa que esse dado não está disponível e deixou-se em -1, uma vez que todos os dados são positivos. Essa escolha é consistente com outras dimensões, que optou-se por marcar -1 toda vez que houvesse uma célula vazia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-photography",
   "metadata": {},
   "source": [
    "Para as outras dimensões, manteve-se o número que estava na entrada, apenas tomando o cuidado de verificar se era um valor válido, se não, registrou-se -1. A dimensão \"Já foi casado\" ao invés de 0 e 1, como nas outras colunas booleanas, tinha \"sim e não\", que foram devidamente substituídas por 0 e 1, mantendo-se a consistência dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-costs",
   "metadata": {},
   "source": [
    "As leituras nesse dataset foram feitas por meio da biblioteca Pandas, que possui uma série de funções adequadas para leitura e manipulação de dados, o que inclui leitura de arquivos CSV (comma separated values), o formato de entrada deste dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-medicine",
   "metadata": {},
   "source": [
    "O dataset fornecido foi lido por meio das funções padrão de leitura de arquivos da liguagem Python, devido ao seu formato de dois números por linha separados por espaço, sem cabeçalho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-hudson",
   "metadata": {},
   "source": [
    "Ambas as fontes de dados foram separadas em conjuntos de aprendizado e teste de tamanho aproximadamente 90% e 10% respectivamente. Isso se deu por meio de escolha aleatória de 10% dos pontos de cada dataset, algoritmo que mantém a distribuição da amostra retirada. Um dos códigos feitos para isso será mostrado abaixo, o código para o outro dataset é análogo e pode ser encontrado junto com todo o código desta etapa do projeto nos notebooks anteriormente anexados.  \n",
    "Um campo cluster, com valor -1, foi adicionado a cada um dos pontos em ambos datasets para utilização em etapas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(\"../data/cluster.dat\")\n",
    "except:\n",
    "    print(\"Houston we've got a problem\")\n",
    "dataset = []\n",
    "testset = []\n",
    "for i in f:\n",
    "    rVar = random.randint(1,10)\n",
    "    stringList = i.split()\n",
    "    try:\n",
    "        dicti = {\n",
    "            \"x\" : float(stringList[0]),\n",
    "            \"y\" : float(stringList[1]),\n",
    "            \"cluster\" : -1\n",
    "        }\n",
    "    except ValueError:\n",
    "        print(\"You had a Value Error\")\n",
    "        break\n",
    "    except:\n",
    "        print(\"You got another Error\")\n",
    "        break\n",
    "    if(rVar == 5):\n",
    "        testset.append(dicti)\n",
    "    else:\n",
    "        dataset.append(dicti)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-understanding",
   "metadata": {},
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-polish",
   "metadata": {},
   "source": [
    "O código desta etapa se encontra no notebook [Kmeans](../Notebooks/kMeans.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-diversity",
   "metadata": {},
   "source": [
    "A primeira coisa feita nesta etapa foi a leitura dos dados no JSON produzido na etapa anterior. O dicionário lido está num formato facilmente compreensível para humanos, porém difícil de se trabalhar na implementação, portanto para esta etapa ele foi salvo numa nova estrutura de dados, um dicionário com dois campos: cluster, onde fica salvo a lista com o código do cluster de cada ponto respectivamente e o segundo campo é matrix, uma matriz em que cada linha representa uma feature e cada coluna representa o valor da feature para o ponto i. O código será mostrado a seguir. Esta estrutura de dados se provou problemática por não manter todas as dimensões de cada ponto numa estrutura só, mas em listas separadas, o que contraria a interface de muitas das funções das bibliotecas usadas. Ela também não usa np.arrays. e sim listas python, que são muito mais ineficientes e difíceis de trabalhar com as funções da biblioteca Numpy, exigindo que várias conversões fossem feitas ao longo do código, o que é altamente ineficiente. Ela foi mantida apenas para não ser necessário refatorar todo o código que já tinha sido feito usando ela, porém foi substituída na etapa seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(dataset):\n",
    "    cleanDataset = {\n",
    "        \"matrix\" : [],\n",
    "        \"cluster\": [],\n",
    "    }\n",
    "    for i in dataset[0]:\n",
    "        if i != \"cluster\":\n",
    "            cleanDataset[\"matrix\"].append(getList(dataset,i))\n",
    "        elif i == \"cluster\":\n",
    "            cleanDataset[\"cluster\"] = getList(dataset,\"cluster\")\n",
    "    return cleanDataset\n",
    "exist = True\n",
    "try:\n",
    "    f = open(\"../data/teacherData.json\",\"r\")\n",
    "    stringSet = f.read()\n",
    "    f.close()\n",
    "except:\n",
    "    print(\"Arquivo não existe\")\n",
    "    exist = False\n",
    "if(exist):\n",
    "    dataset = json.loads(stringSet)\n",
    "    nDataset = getDataset(dataset)\n",
    "    plt.scatter(nDataset[\"matrix\"][0],nDataset[\"matrix\"][1])\n",
    "    plt.title(\"Original Data\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../images/originalData1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-today",
   "metadata": {},
   "source": [
    "O código da nossa implementação do Kmeans será anexado em seguida. A distância utilizada foi a distância Euclidiana e ass estratégias de inicialização foram Forgy e - random partition -, que serão explicadas mais adiante. Todas as funções auxiliares podem ser encontradas no notebook anteriormente mencionado e não serão anexadas para manter a concisão deste relatório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "patient-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(dataset,k):\n",
    "    centers = [] \n",
    "    centers = initialize(dataset[\"matrix\"],k)\n",
    "    for i in range(len(dataset[\"cluster\"])):\n",
    "        dataset[\"cluster\"][i] = getCluster(getPoint(dataset[\"matrix\"],i),centers)\n",
    "    k = 0\n",
    "    oldCenters = copy.deepcopy(centers)\n",
    "    while(True):\n",
    "        for i in range(len(centers)):\n",
    "            newCenter = getCenter(getClusterPoints(dataset,i))\n",
    "            if newCenter == None:\n",
    "                centers[i] = [0]*len(centers[i])\n",
    "                continue\n",
    "            centers[i] = newCenter\n",
    "        for i in range(len(dataset[\"cluster\"])):\n",
    "            dataset[\"cluster\"][i] = getCluster(getPoint(dataset[\"matrix\"],i),centers)\n",
    "        k+=1\n",
    "        if k > MAXIT:\n",
    "            break\n",
    "        elif (not change(centers,oldCenters)) and k > 0:\n",
    "            break\n",
    "        oldCenters = copy.deepcopy(centers)\n",
    "    print(k)\n",
    "    dataset[\"centers\"] = centers\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-rebound",
   "metadata": {},
   "source": [
    "Este código segue os princípios do Kmeans, ele inicializa o dataset seguindo as estratégias adotadas, então a cada iteração ele encontra novos centros para cada cluster e depois atualiza o cluster de cada ponto, parando quando todos os centros de cada cluster não se moverem mais, dentro de uma margem de erro pré estabelecida. Também foi estabelecido um valor máximo de iterações permitidas, para evitar a situação de um loop infinito no caso de um bug passar despercebido no código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-swaziland",
   "metadata": {},
   "source": [
    "## Efeito da normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-reserve",
   "metadata": {},
   "source": [
    "Observou-se que esse algoritmo é altamente sensível à diferença na ordem de grandeza entre as dimensões. O algoritmo produziu resultados diferentes do esparado com os dados não normalizados e serão comparados com e sem normalização para o dataset fornecido, para o outro dataset a análise fica muito mais complicada por não se poder plotar gráficos de pontos do resultado e então optou-se por trabalhar apenas com os dados normalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-actor",
   "metadata": {},
   "source": [
    "![Original data](../images/originalData1.jpg)  \n",
    "<b>Imagem 3:</b> Dados fornecidos sem normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-mechanism",
   "metadata": {},
   "source": [
    "![No Norm result](../images/clusteredData1n.jpg)  \n",
    "<b>Imagem 4:</b> Resultado sem normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-fitness",
   "metadata": {},
   "source": [
    "![No Norm Elbow](../images/Elbow1n.jpg)  \n",
    "<b>Imagem 5:</b> Método de Elbow aplicado nos dados não normalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-legend",
   "metadata": {},
   "source": [
    " ![Norm result](../images/clusteredData1.jpg)  \n",
    "<b>Imagem 6:</b> Resultado com dados normalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-stranger",
   "metadata": {},
   "source": [
    "![Norm Elbow](../images/Elbow1.jpg)  \n",
    "<b>Imagem 7:</b> Método de Elbow aplicado nos dados normalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-relationship",
   "metadata": {},
   "source": [
    "Como observado, uma das dimensões é várias ordens de grandeza superior à outra, o que acaba enviesando a distância euclidiana. A imagem possui três grupos claros e distintos, o algoritmo conseguiu encontrar esses três clusters para os dados normalizados, porém encontrou grupos errados para os dados não normalizados. Ambas as análisess de Elbow ficaram com o mesmo formato, porém a função de custo para os dados não normalizados possui uma seção quase vertical que vai até 1e8 entre k = 1 e k = 2, o que não acontece com os dados normalizados, que também apresentam custos menores em toda a curva quando comparados com os dados não normalizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-phone",
   "metadata": {},
   "source": [
    "Mesmo utilizando a inicialização forgy, que é aleatória, obteve-se o mesmo resultado em todas as execuções e também observou-se uma diferença nas quantidades de iterações que podem ser observadas na tabela a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-incentive",
   "metadata": {},
   "source": [
    "### Tabela número de iterações:\n",
    "\n",
    "\n",
    "k | Não normalizado | Normalizado\n",
    "- | --------------- | -----------\n",
    "1 | 2               | 2\n",
    "2 | 9               | 4\n",
    "3 | 8               | 4\n",
    "4 | 12              | 6\n",
    "5 | 14              | 10\n",
    "6 | 8               | 7\n",
    "7 | 10              | 9\n",
    "8 | 21              | 12\n",
    "9 | 10              | 10\n",
    "10| 34              | 10\n",
    "<b>Tabela 1:</b> Número de iterações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-saudi",
   "metadata": {},
   "source": [
    "Para cada k o número de iterações é igual ou menor no normalizado em relação ao não normalizado, chegando a apresentar mais de dez iterações de diferença para alguns ks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-subscriber",
   "metadata": {},
   "source": [
    "Desta forma é possível perceber que a análise com dados normalizados é uma opção melhor do que com dados não normalizados. A estratégia de normalização está no código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    norm = np.linalg.norm(np.array(array))\n",
    "    for i in range(len(array)):\n",
    "        array[i] = array[i]/norm\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-semester",
   "metadata": {},
   "source": [
    "Este algoritmo é aplicado para cada lista de features do dataset e a normalização é feita dividindo-se cada elemento da lista pela norma dois da lista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-keyboard",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-junior",
   "metadata": {},
   "source": [
    "As imagens com alguns resultados da aplicação sobre os dados fornecidos foram apresentados na seção anterior e serão discutidos juntamente com os outros resultados agora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-storm",
   "metadata": {},
   "source": [
    "![Original data](../images/originalData1.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-collaboration",
   "metadata": {},
   "source": [
    "Os dados fornecidos possuem duas dimensões, aqui chamadas de x e y. Eles estão Disstribuídos numa faixa de 250 a 3750, aproximadamente em x e de 0 a 30, aproximadamente, em y. Visualmente é possível perceber 3 grupos distintos, que podem representar 3 diferentes clusters após a execução do algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-pakistan",
   "metadata": {},
   "source": [
    "Ao executar o algoritmo, utilizamos o método de Elbow para escolher o melhor valor de K como parâmetro para o algoritmo e assim obter o melhor modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-interview",
   "metadata": {},
   "source": [
    "O código do método de Elbow será anexador a seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow(dataset,path,r):\n",
    "    ks = []\n",
    "    Js = []\n",
    "    results = []\n",
    "    for k in range(1,r+1):\n",
    "        dataset = kmeans(dataset,k)\n",
    "        results.append(copy.deepcopy(dataset))\n",
    "        Js.append(getObjectiveFunction(3,dataset[\"centers\"],dataset))\n",
    "        ks.append(k)\n",
    "        dataset[\"cluster\"] = [-1]*len(dataset[\"matrix\"][0])\n",
    "    plt.plot(ks,Js,marker = \"o\")\n",
    "    plt.title(\"Elbow Method\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Objective Function\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(path) \n",
    "    return results\n",
    "def getObjectiveFunction(k,centers,dataset):\n",
    "    J = 0\n",
    "    for i in range(k):\n",
    "        clusterPoints = getClusterPoints(dataset,i)\n",
    "        for t in clusterPoints:\n",
    "            J += math.pow(getDistance(t,centers[i]),2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-future",
   "metadata": {},
   "source": [
    "Este método se baseia no cálculo da função objetiva, ou função de custo para cada K. A fórmula que calcula essa função é:  \n",
    "![Objective Function](ObjectiveeFunction.png)  \n",
    "<b>Imagem 8:</b> Fórmula de função objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-illustration",
   "metadata": {},
   "source": [
    "O resultado da aplicação deste método para os dados normalizados foi exibido anteriormente e será exibido novamente adiante:  \n",
    "![Norm Elbow](../images/Elbow1.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-groove",
   "metadata": {},
   "source": [
    "Este gráfico mostra que o custo cai conforme K aumenta, no entanto, a partir de um ponto de inflexão (k = 3) a queda do valor de J muda de comportamento, deixa de ser um reta e se torna um valor quase constante, logo, não importa quanto aumentamos K, o custo se mantém praticamento o mesmo. Desta forma, podemos concluir que K = 3 é a melhor opção para o parâmetro K, a opção que melhor se adaptará aos dados, sem provocar o fenômeno de Overfitting.  \n",
    "O algoritmo foi executado com K = 3, o resultado será exibido a seguir:  \n",
    "![Norm result](../images/clusteredData1.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-horse",
   "metadata": {},
   "source": [
    "O gráfico mostra que o algoritmo conseguiu aglomerar os dados nos trẽs clusters que eram visualmente percebidos, com todos os pontos que aparentemente pertenciam a esses clusters sendo designados para os clusters corretos. O custo desta execução foi inferior a 0.1, isto siginifica que a soma das distâncias entre cada ponto e o centro de seu cluster ao quadrado é inferior 0.1. A quantidade de iterações muda a cada execução devido à inicilização aleatória, porém para esta execução registrada, quantidade de iterações foi 4, como exibido pela tabela 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
